{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spark_on_Local_Computers.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOZo0cGFBv6Ad/+oNAuSgFf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amjadraza/spark-ml-course/blob/main/Spark_on_Local_Computers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JCIUZNbhFGw"
      },
      "source": [
        "## **How to use Spark on your local computer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAHg3AIxhW_o"
      },
      "source": [
        "#### Using Conda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wjt625gdhecf"
      },
      "source": [
        "Conda is an open-source package management and environment management system which is a part of the Anaconda distribution. It is both cross-platform and language agnostic. In practice, Conda can replace both pip and virtualenv. You can download the anaconda packages using the link [here](https://www.anaconda.com/products/individual). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfmADWOQhg7q"
      },
      "source": [
        "1. Anaconda for windows:\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/88575776/130860975-dd48e887-221a-41c8-8221-96c08729ad24.png)\n",
        "\n",
        "2. For Linux and Mac OS, we have the following options available for downloading anaconda presented at the end of the same page.\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/88575776/130861215-d4098a13-808e-45b6-85fe-8424d2efff2d.png)\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjLaPRvyhpr5"
      },
      "source": [
        "After downloading Anaconda, you have to create a conda environment. Open your terminal and use the command shown below.\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/88575776/130861971-c2e2cb5d-586f-4f9f-b11d-5cd25a679551.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQU2QYDQh5LS"
      },
      "source": [
        "After the virtual environment is created, it should be visible under the list of Conda environments which can be seen using the following command:\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/88575776/130862239-94a8ecd9-3da2-443e-a868-953ae24ee0f8.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkTYxxwliGIE"
      },
      "source": [
        "Now activate the newly created environment with the following command:\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/88575776/130862521-c41a179d-2893-4431-8569-5b2fcc33fbb2.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmazFnqviJVl"
      },
      "source": [
        "You can install pyspark by Using PyPI to install PySpark in the newly created environment, for example as below. It will install PySpark under the new virtual environment pyspark_env created above.\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/88575776/130862820-47765ca2-56f3-4bff-92cf-f0c31e6929b7.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yH10YB6iOaU"
      },
      "source": [
        "Alternatively, you can install PySpark from Conda itself as below:\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/88575776/130863174-989bbc81-a5bd-4424-b653-be85a9aae226.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NXA0GVJiVe1"
      },
      "source": [
        "**Manually Downloading PySpark**\n",
        "\n",
        "PySpark is included in the distributions available at the [Apache Spark website](https://spark.apache.org/downloads.html). You can download a distribution you want from the site. After that, uncompress the tar file into the directory where you want to install Spark, for example, as below:\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/88575776/130863652-e021cffd-6be3-4c88-acbe-8ce1cececbf5.png)\n",
        "\n",
        "Ensure the SPARK_HOME environment variable points to the directory where the tar file has been extracted. Update PYTHONPATH environment variable such that it can find the PySpark and Py4J under SPARK_HOME/python/lib. One example of doing this is shown below:\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/88575776/130863797-8385ac42-d06d-4e11-8551-3ce328ad0cc5.png)"
      ]
    }
  ]
}
