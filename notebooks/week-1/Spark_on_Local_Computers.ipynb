{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spark_on_Local_Computers.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNMCvHIhNwtZuuxbDmtnsJJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amjadraza/learn-ml-with-spark/blob/main/Spark_on_Local_Computers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JCIUZNbhFGw"
      },
      "source": [
        "## **How to use Spark on your local computer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAHg3AIxhW_o"
      },
      "source": [
        "#### Using Conda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wjt625gdhecf"
      },
      "source": [
        "Conda is an open-source package management and environment management system which is a part of the Anaconda distribution. It is both cross-platform and language agnostic. In practice, Conda can replace both pip and virtualenv. You can download the anaconda packages using the link [here](https://www.anaconda.com/products/individual). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfmADWOQhg7q"
      },
      "source": [
        "1. Anaconda for windows:\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/88575776/130860975-dd48e887-221a-41c8-8221-96c08729ad24.png)\n",
        "\n",
        "2. For Linux and Mac OS, we have the following options available for downloading anaconda presented at the end of the same page.\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/88575776/130861215-d4098a13-808e-45b6-85fe-8424d2efff2d.png)\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjLaPRvyhpr5"
      },
      "source": [
        "After downloading Anaconda, you have to create a conda environment. Open your terminal and use the command shown below.\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/88575776/130861971-c2e2cb5d-586f-4f9f-b11d-5cd25a679551.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQU2QYDQh5LS"
      },
      "source": [
        "After the virtual environment is created, it should be visible under the list of Conda environments which can be seen using the following command:\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/88575776/130862239-94a8ecd9-3da2-443e-a868-953ae24ee0f8.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkTYxxwliGIE"
      },
      "source": [
        "Now activate the newly created environment with the following command:\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/88575776/130862521-c41a179d-2893-4431-8569-5b2fcc33fbb2.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmazFnqviJVl"
      },
      "source": [
        "You can install pyspark by Using PyPI to install PySpark in the newly created environment, for example as below. It will install PySpark under the new virtual environment pyspark_env created above.\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/88575776/130862820-47765ca2-56f3-4bff-92cf-f0c31e6929b7.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yH10YB6iOaU"
      },
      "source": [
        "Alternatively, you can install PySpark from Conda itself as below:\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/88575776/130863174-989bbc81-a5bd-4424-b653-be85a9aae226.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NXA0GVJiVe1"
      },
      "source": [
        "**Manually Downloading PySpark**\n",
        "\n",
        "PySpark is included in the distributions available at the [Apache Spark website](https://spark.apache.org/downloads.html). You can download a distribution you want from the site. After that, uncompress the tar file into the directory where you want to install Spark, for example, as below:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDAgCvB7czES",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9abf381-386c-4ed1-b698-ef9f85b9b01a"
      },
      "source": [
        "#!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!java -version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"11.0.11\" 2021-04-20\n",
            "OpenJDK Runtime Environment (build 11.0.11+9-Ubuntu-0ubuntu2.18.04)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.11+9-Ubuntu-0ubuntu2.18.04, mixed mode, sharing)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q http://apache.osuosl.org/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz"
      ],
      "metadata": {
        "id": "EJWXOC5z7c4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar xf spark-3.2.0-bin-hadoop3.2.tgz"
      ],
      "metadata": {
        "id": "CMeH3tin7iTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensure the SPARK_HOME environment variable points to the directory where the tar file has been extracted. Update PYTHONPATH environment variable such that it can find the PySpark and Py4J under SPARK_HOME/python/lib. One example of doing this is shown below:\n"
      ],
      "metadata": {
        "id": "s_2dD3zW7_Jo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.0-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "FVoEKyzO7r0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "EBfJzTQZ8Dw8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
